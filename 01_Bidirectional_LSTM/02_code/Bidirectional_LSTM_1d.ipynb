{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                       Adj Close       Close        High         Low  \\\n",
      "Date                                                                        \n",
      "2022-01-03 00:00:00+00:00  179.273621  182.009995  182.880005  177.710007   \n",
      "2022-01-04 00:00:00+00:00  176.998337  179.699997  182.940002  179.119995   \n",
      "2022-01-05 00:00:00+00:00  172.290192  174.919998  180.169998  174.639999   \n",
      "2022-01-06 00:00:00+00:00  169.414093  172.000000  175.300003  171.639999   \n",
      "2022-01-07 00:00:00+00:00  169.581558  172.169998  174.139999  171.029999   \n",
      "\n",
      "Price                            Open     Volume  \n",
      "Date                                              \n",
      "2022-01-03 00:00:00+00:00  177.830002  104487900  \n",
      "2022-01-04 00:00:00+00:00  182.630005   99310400  \n",
      "2022-01-05 00:00:00+00:00  179.610001   94537600  \n",
      "2022-01-06 00:00:00+00:00  172.699997   96904000  \n",
      "2022-01-07 00:00:00+00:00  172.889999   86709100  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2022-01-03\"\n",
    "end_date   = \"2024-11-01\"\n",
    "\n",
    "data = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "data.columns = data.columns.get_level_values(0) # from multi index to single index\n",
    "print(data.head())\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['days_range'] = data['High'] - data['Low']   # calculating the range of the day\n",
    "# data['yesterdays_close'] = data['Adj Close'].shift(1)\n",
    "# data['jump_from_yesterday'] = data['Open']- data['yesterdays_close']\n",
    "# data['days_movement'] = data['Adj Close']-data['Open']\n",
    "# data = data.drop(columns=['yesterdays_close','Close','High','Low','Open'])\n",
    "\n",
    "# data_v1 = data.copy()\n",
    "# del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_copy.copy()\n",
    "\n",
    "data['target'] = data['Adj Close'].shift(-1)\n",
    "data = data[data['target'].notnull()]\n",
    "data = data.drop(columns=['Close','High','Low','Open','Volume'])\n",
    "data = data.reset_index()\n",
    "data['Date'] = data['Date'].dt.strftime('%Y-%m-%d')\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "data.set_index('Date', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>179.273621</td>\n",
       "      <td>176.998337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>176.998337</td>\n",
       "      <td>172.290192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>172.290192</td>\n",
       "      <td>169.414093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>169.414093</td>\n",
       "      <td>169.581558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>169.581558</td>\n",
       "      <td>169.601257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        Adj Close      target\n",
       "Date                              \n",
       "2022-01-03  179.273621  176.998337\n",
       "2022-01-04  176.998337  172.290192\n",
       "2022-01-05  172.290192  169.414093\n",
       "2022-01-06  169.414093  169.581558\n",
       "2022-01-07  169.581558  169.601257"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume start_date and end_date are defined, and data is your DataFrame\n",
    "a = pd.date_range(start=start_date, end=end_date, freq=\"D\")  # continuous dates\n",
    "b = data.index  # our time series\n",
    "diff_dates = a.difference(b)  # finds what in 'a' is not in 'b'\n",
    "\n",
    "# Ensure diff_dates remains as Timestamps for compatibility with DataFrame index\n",
    "# (no need to convert to string and back to datetime)\n",
    "diff_dates = pd.to_datetime(diff_dates).date\n",
    "\n",
    "# Ensure `td` is defined correctly\n",
    "td = pd.Timedelta(days=1)  # Adjust according to your needs\n",
    "\n",
    "for date in diff_dates:\n",
    "    prev_date = date - td  # Previous date\n",
    "    # Check if the previous date exists in the index\n",
    "    if prev_date in data.index:  # prev_date is still a Timestamp\n",
    "        prev_val = data.loc[prev_date]  # Access using loc\n",
    "        data.loc[date] = prev_val  # Impute previous value\n",
    "    else:\n",
    "        print(f\"Previous date {prev_date} not found in index.\")  # Debug message or handling\n",
    "\n",
    "data.sort_index(inplace=True)  # Sort the index\n",
    "data.freq = \"D\"  # Set the time index frequency as daily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split date and convert to date-only format to match the index\n",
    "val_split_date = pd.to_datetime('2023-12-31').date()\n",
    "test_split_date = pd.to_datetime('2024-06-30').date()\n",
    "\n",
    "# Split the data\n",
    "train = data[:val_split_date]  # Data up to and including 2023-12-31\n",
    "val = data[val_split_date:test_split_date]   # Data from 2023-12-31 onward\n",
    "test = data[test_split_date:]   # Data from 2023-12-31 onward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler_train = MinMaxScaler()\n",
    "values = scaler_train.fit_transform(train[['Adj Close']])\n",
    "\n",
    "\n",
    "# Define the window size\n",
    "WINDOW = 14  # Window size of 14 days\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(values) # Create a TensorFlow Dataset from the array\n",
    "train_data = train_data.window(WINDOW + 1, shift=1, drop_remainder=True) # Create windowed dataset with the specified window size\n",
    "train_data = train_data.flat_map(lambda x: x.batch(WINDOW + 1)) # Flatten the windowed dataset by batching\n",
    "\n",
    "# Create features and target tuple\n",
    "train_data = train_data.map(lambda x: (x[:-1], x[-1]))  # x[:-1] for features, x[-1, 1] for target 'Adj Close' # Here, we use all columns for features, but only the 'Adj Close' (index 1) as the target\n",
    "train_data = train_data.batch(32).prefetch(1) # Create batches of windows\n",
    "\n",
    "\n",
    "\n",
    "scaler_val = MinMaxScaler()\n",
    "val_values = scaler_val.fit_transform(val[[ 'Adj Close']])\n",
    "\n",
    "# Convert to TensorFlow Datasets similarly as before\n",
    "#val_values = val[['Volume', 'Adj Close']].values\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val_values)\n",
    "val_data = val_data.window(WINDOW + 1, shift=1, drop_remainder=True).flat_map(lambda x: x.batch(WINDOW + 1))\n",
    "val_data = val_data.map(lambda x: (x[:-1], x[-1])).batch(32).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Conda_3_12_7/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_16                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">133,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_17                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_18                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_16                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m133,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_17                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_18                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">990,721</span> (3.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m990,721\u001b[0m (3.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">989,185</span> (3.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m989,185\u001b[0m (3.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Custom callback\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('mae') < 0.1:\n",
    "            print(\"MAE under 0.1... Stopping training\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "my_callback = CustomCallback()\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return lr * 0.99\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# LSTM model definition\n",
    "lstm_model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True), input_shape=[WINDOW,1]),\n",
    "    Dropout(0.1),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(128, return_sequences=True), input_shape=[WINDOW,1]),\n",
    "    Dropout(0.1),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.1),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.1),  # Experiment with different dropout rates\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    loss=Huber(),\n",
    "    optimizer=Adam(),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Model summary\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "     21/Unknown \u001b[1m3s\u001b[0m 26ms/step - loss: 3.8053 - mae: 2.0083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Conda_3_12_7/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 3.6673 - mae: 1.9060 - val_loss: 1.3525 - val_mae: 0.5881 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.9403 - mae: 0.2613 - val_loss: 0.5434 - val_mae: 0.5880 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.3292 - mae: 0.3378 - val_loss: 0.1182 - val_mae: 0.2054 - learning_rate: 0.0099\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1093 - mae: 0.2310 - val_loss: 0.0694 - val_mae: 0.2149 - learning_rate: 0.0098\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0623 - mae: 0.2069 - val_loss: 0.0451 - val_mae: 0.2040 - learning_rate: 0.0097\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0372 - mae: 0.1838 - val_loss: 0.0426 - val_mae: 0.2177 - learning_rate: 0.0096\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0316 - mae: 0.1826 - val_loss: 0.0407 - val_mae: 0.2215 - learning_rate: 0.0095\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0274 - mae: 0.1757 - val_loss: 0.0416 - val_mae: 0.2288 - learning_rate: 0.0094\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0264 - mae: 0.1739 - val_loss: 0.0413 - val_mae: 0.2303 - learning_rate: 0.0093\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0251 - mae: 0.1728 - val_loss: 0.0417 - val_mae: 0.2338 - learning_rate: 0.0092\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0243 - mae: 0.1712 - val_loss: 0.0418 - val_mae: 0.2361 - learning_rate: 0.0091\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0236 - mae: 0.1700 - val_loss: 0.0423 - val_mae: 0.2386 - learning_rate: 0.0090\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0225 - mae: 0.1657 - val_loss: 0.0425 - val_mae: 0.2404 - learning_rate: 0.0045\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0223 - mae: 0.1666 - val_loss: 0.0429 - val_mae: 0.2426 - learning_rate: 0.0044\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0223 - mae: 0.1670 - val_loss: 0.0430 - val_mae: 0.2433 - learning_rate: 0.0044\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0222 - mae: 0.1664 - val_loss: 0.0430 - val_mae: 0.2434 - learning_rate: 0.0043\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0224 - mae: 0.1673 - val_loss: 0.0430 - val_mae: 0.2435 - learning_rate: 0.0043\n"
     ]
    }
   ],
   "source": [
    "# lstm_history = lstm_model.fit(\n",
    "#     train_data,\n",
    "#     epochs=100,\n",
    "#     callbacks=[lr_scheduler, my_callback]\n",
    "# )\n",
    "\n",
    "# Fit the model with training data\n",
    "lstm_history = lstm_model.fit(\n",
    "    train_data,\n",
    "    epochs=100,\n",
    "    validation_data=val_data,  # Ensure you define val_data appropriately\n",
    "    callbacks=[lr_scheduler, my_callback, early_stopping, reduce_lr],\n",
    "    batch_size=32  # Optional: specify batch size if it's not handled in your Dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /Users/monilshah/Documents/02_NWU/09_MSDS_458_DL/99_group_project/StockPricePrediction/01_Bidirectional_LSTM/01_Models/bidirecttional_lstm_1d.keras\n"
     ]
    }
   ],
   "source": [
    "# Assuming lstm_model is your trained model\n",
    "model_save_path = '/Users/monilshah/Documents/02_NWU/09_MSDS_458_DL/99_group_project/StockPricePrediction/01_Bidirectional_LSTM/01_Models/bidirecttional_lstm_1d.keras'  # Specify your desired path here\n",
    "\n",
    "# Save the model\n",
    "lstm_model.save(model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'test' is your test DataFrame and scaler_train is your fitted MinMaxScaler\n",
    "\n",
    "# Scale the test data\n",
    "test_values = scaler_val.transform(test[['Adj Close']])  # Use only the features for scaling\n",
    "\n",
    "# Define the window size\n",
    "WINDOW = 30  # Same window size as used for training\"\n",
    "\n",
    "# Create a TensorFlow Dataset from the scaled test data\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test_values)\n",
    "\n",
    "# Create windowed dataset with the specified window size\n",
    "test_data = test_data.window(WINDOW + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windowed dataset by batching\n",
    "test_data = test_data.flat_map(lambda x: x.batch(WINDOW + 1))\n",
    "\n",
    "# Create features and target tuple (for evaluation, you can use the last value as target)\n",
    "test_data = test_data.map(lambda x: (x[:-1], x[-1]))  # Here, x[-1, 1] corresponds to the 'Adj Close' price\n",
    "\n",
    "# Create batches of windows\n",
    "test_data = test_data.batch(32).prefetch(1)  # Adjust batch size as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Conda_3_12_7/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    }
   ],
   "source": [
    "predictions = lstm_model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47606865],\n",
       "       [0.47610283],\n",
       "       [0.47612596],\n",
       "       [0.47613373],\n",
       "       [0.47614127],\n",
       "       [0.47618377],\n",
       "       [0.47618034],\n",
       "       [0.47619095],\n",
       "       [0.4762174 ],\n",
       "       [0.47624117],\n",
       "       [0.4762747 ],\n",
       "       [0.47622025],\n",
       "       [0.47624543],\n",
       "       [0.4762488 ],\n",
       "       [0.47626638],\n",
       "       [0.47631395],\n",
       "       [0.47630706],\n",
       "       [0.47623634],\n",
       "       [0.4761938 ],\n",
       "       [0.47619587],\n",
       "       [0.47620165],\n",
       "       [0.4762061 ],\n",
       "       [0.47620672],\n",
       "       [0.47620228],\n",
       "       [0.47614694],\n",
       "       [0.47614056],\n",
       "       [0.47614294],\n",
       "       [0.47614062],\n",
       "       [0.47614446],\n",
       "       [0.476151  ],\n",
       "       [0.47616228],\n",
       "       [0.47618335],\n",
       "       [0.47616386],\n",
       "       [0.4761776 ],\n",
       "       [0.47617358],\n",
       "       [0.47614685],\n",
       "       [0.476076  ],\n",
       "       [0.47606707],\n",
       "       [0.47607803],\n",
       "       [0.47609466],\n",
       "       [0.47610557],\n",
       "       [0.4761    ],\n",
       "       [0.47609702],\n",
       "       [0.47611073],\n",
       "       [0.47613955],\n",
       "       [0.47614872],\n",
       "       [0.47617853],\n",
       "       [0.47619054],\n",
       "       [0.47619063],\n",
       "       [0.4761933 ],\n",
       "       [0.47619516],\n",
       "       [0.47620073],\n",
       "       [0.4761927 ],\n",
       "       [0.47617364],\n",
       "       [0.4761905 ],\n",
       "       [0.47619048],\n",
       "       [0.47619206],\n",
       "       [0.47620097],\n",
       "       [0.47621086],\n",
       "       [0.47620675],\n",
       "       [0.47624087],\n",
       "       [0.47623867],\n",
       "       [0.4762445 ],\n",
       "       [0.4762471 ],\n",
       "       [0.47623038],\n",
       "       [0.47616717],\n",
       "       [0.47615445],\n",
       "       [0.47616574],\n",
       "       [0.4761526 ],\n",
       "       [0.47614962],\n",
       "       [0.47614792],\n",
       "       [0.47614908],\n",
       "       [0.4761476 ],\n",
       "       [0.4761655 ],\n",
       "       [0.4761641 ],\n",
       "       [0.47616082],\n",
       "       [0.47615644],\n",
       "       [0.4761358 ],\n",
       "       [0.47608507],\n",
       "       [0.4760906 ],\n",
       "       [0.47613567],\n",
       "       [0.47621578],\n",
       "       [0.47620952],\n",
       "       [0.4762089 ],\n",
       "       [0.47620416],\n",
       "       [0.4761871 ],\n",
       "       [0.4761918 ],\n",
       "       [0.4761857 ],\n",
       "       [0.47619754],\n",
       "       [0.47620285],\n",
       "       [0.4762049 ],\n",
       "       [0.47621846],\n",
       "       [0.47625756],\n",
       "       [0.47618315],\n",
       "       [0.47618517]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189.26230082 189.26407266 189.26527139 189.26567458 189.2660654\n",
      " 189.26826823 189.26809058 189.26864051 189.27001071 189.27124343\n",
      " 189.27298128 189.27015901 189.27146433 189.27163889 189.2725503\n",
      " 189.27501573 189.27465889 189.27099318 189.26878726 189.2688954\n",
      " 189.26919508 189.26942525 189.26945769 189.26922752 189.2663589\n",
      " 189.26602833 189.26615191 189.26603142 189.26623069 189.26656899\n",
      " 189.26715445 189.2682466  189.26723633 189.26794846 189.26773992\n",
      " 189.26635427 189.26268238 189.26221895 189.26278742 189.2636494\n",
      " 189.26421478 189.26392591 189.26377143 189.26448202 189.2659758\n",
      " 189.26645159 189.26799635 189.26861889 189.26862352 189.26876255\n",
      " 189.26885832 189.26914719 189.26873165 189.26774301 189.26861734\n",
      " 189.2686158  189.26869767 189.26915955 189.26967241 189.26945923\n",
      " 189.27122798 189.27111367 189.27141644 189.27155084 189.27068423\n",
      " 189.2674078  189.26674818 189.26733365 189.26665241 189.26649793\n",
      " 189.26640988 189.26647013 189.26639289 189.26732129 189.26724868\n",
      " 189.26707876 189.26685168 189.26578116 189.26315199 189.26343931\n",
      " 189.26577499 189.2699273  189.2696029  189.26957046 189.26932484\n",
      " 189.26844124 189.26868377 189.26836863 189.2689819  189.26925687\n",
      " 189.26936346 189.27006632 189.27209305 189.26823579 189.26834083]\n"
     ]
    }
   ],
   "source": [
    "# Reshape predictions to match the expected input shape of the scaler\n",
    "predictions_reshaped = np.zeros((predictions.shape[0], 2))  # Create an array for two features\n",
    "predictions_reshaped[:, 0] = predictions.flatten()  # Place the predictions in the first column\n",
    "\n",
    "# Inverse scale the predictions\n",
    "predictions_original = scaler_val.inverse_transform(predictions_reshaped)\n",
    "\n",
    "# Extract the first column for actual sales predictions\n",
    "predictions_original = predictions_original[:, 0]\n",
    "\n",
    "# Output the original predictions\n",
    "print(predictions_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2024-06-30    210.376480\n",
       "2024-07-01    216.499405\n",
       "2024-07-02    220.015335\n",
       "2024-07-03    221.293854\n",
       "2024-07-04    221.293854\n",
       "                 ...    \n",
       "2024-10-28    233.399994\n",
       "2024-10-29    233.669998\n",
       "2024-10-30    230.100006\n",
       "2024-10-31    230.100006\n",
       "2024-11-01    230.100006\n",
       "Name: Adj Close, Length: 125, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test['Adj Close']\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda_3_12_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
