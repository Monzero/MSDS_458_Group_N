{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Dates                                               News\n",
      "0  13-09-2017          gold suffers third straight daily decline\n",
      "1  28-02-2018  dent research : is gold's day in the sun comin...\n",
      "2  06-09-2017  Gold snaps three-day rally as Trump, lawmakers...\n",
      "3  16-03-2018  april gold holds slight gain, up $2.50, or 0.2...\n",
      "4  28-07-2017  gold trades in red in early trade; eyes near-t...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the path to your CSV file\n",
    "#file_path1 = 'analyst_ratings_processed.csv'\n",
    "file_path1 = 'GoldPrice.csv'\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_19552\\190000126.py\", line 2, in <module>\n",
      "    from transformers import BertTokenizer, BertForSequenceClassification\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 896, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 895, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 905, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 25, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\__init__.py\", line 870, in <module>\n",
      "    from . import _masked\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\_masked\\__init__.py\", line 420, in <module>\n",
      "    def sum(input: Tensor,\n",
      "  File \"c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\_masked\\__init__.py\", line 223, in _apply_docstring_templates\n",
      "    example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])\n",
      "c:\\Users\\akash\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\_masked\\__init__.py:223: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 252/252 [00:00<00:00, 470kB/s]\n",
      "Downloading: 100%|██████████| 758/758 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 468kB/s] \n",
      "Downloading: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 418M/418M [00:57<00:00, 7.58MB/s] \n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "#Getting the tokenizer and the model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(news_text):\n",
    "    tokens = tokenizer.encode_plus(news_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = softmax(outputs.logits, dim=-1)  # Get probabilities\n",
    "        label = torch.argmax(probs, dim=1).item()  # Get label (0, 1, or 2)\n",
    "    \n",
    "    # Map labels to sentiments\n",
    "    label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    return label_map[label], probs.numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on the 'news' column\n",
    "df[\"sentiment\"], df[\"probabilities\"] = zip(*df[\"News\"].apply(analyze_sentiment))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gold.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
